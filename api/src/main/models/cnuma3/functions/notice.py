"""
ì¶©ë‚¨ëŒ€í•™êµ ìž¬í•™ìƒì„ ìœ„í•œ ì „ìš© Functions
Chungnam National University specific functions for students
"""

import requests
from typing import Dict, Any
from datetime import datetime, timedelta
import sys
import os
import re

try:
    from ...utils import web_search
except ImportError:
    parent = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    sys.path.insert(0, parent)
    from utils import web_search


class CNUWebAPI:
    """ì¶©ë‚¨ëŒ€í•™êµ ì›¹ì‚¬ì´íŠ¸ API í´ëž˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ì¶©ë‚¨ëŒ€ ì£¼ìš” ì‚¬ì´íŠ¸ URL
        self.plus_url = "https://plus.cnu.ac.kr"
        self.ai_dept_url = "https://ai.cnu.ac.kr"
        
        # ì£¼ìš” íŽ˜ì´ì§€ URL ë§¤í•‘
        self.page_urls = {
            "academic_calendar_undergrad": "/_prog/academic_calendar/?site_dvs_cd=kr&menu_dvs_cd=05020101",
            "academic_calendar_grad": "/_prog/academic_calendar/?site_dvs_cd=kr&menu_dvs_cd=05020102", 
            "shuttle_bus": "/html/kr/sub05/sub05_050403.html",
            "graduation_requirements": "/html/kr/sub02/",  # ìˆ˜ì •ëœ URL
            "notices": "/html/kr/sub04/sub04_040101.html",  # ìˆ˜ì •ëœ URL
            "ai_notices": "/bbs/board.php?bo_table=notice"
        }

    def fetch_page_content(self, url: str) -> Dict[str, Any]:
        """ì›¹íŽ˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê³  íŒŒì‹±"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            result = {
                "url": url,
                "status_code": response.status_code,
                "title": "",
                "content": "",
                "raw_html": response.text
            }
            
            # BeautifulSoupë¡œ íŒŒì‹±
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title_tag = soup.find('title')
                if title_tag:
                    result["title"] = title_tag.get_text(strip=True)
                
                # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ì œê±°)
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                
                # í…ìŠ¤íŠ¸ ë‚´ìš© ì •ë¦¬
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                result["content"] = ' '.join(chunk for chunk in chunks if chunk)
                
            except ImportError:
                result["content"] = "BeautifulSoup4ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install beautifulsoup4"
            
            return result
            
        except Exception as e:
            return {"error": f"íŽ˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"}


# ì „ì—­ CNU API ì¸ìŠ¤í„´ìŠ¤
cnu_api = CNUWebAPI()


def get_cnu_notices(source: str = "ëŒ€í•™", max_results: int = 10) -> str:
    """
    ì¶©ë‚¨ëŒ€í•™êµ ê³µì§€ì‚¬í•­ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Args:
        source: ê³µì§€ì‚¬í•­ ì†ŒìŠ¤ (ëŒ€í•™, ì¸ê³µì§€ëŠ¥í•™ê³¼)
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
    
    Returns:
        ê³µì§€ì‚¬í•­ ëª©ë¡ ë¬¸ìžì—´
    """
    try:
        if source == "ì¸ê³µì§€ëŠ¥í•™ê³¼":
            base_url = cnu_api.ai_dept_url
            notice_path = cnu_api.page_urls["ai_notices"]
        else:
            base_url = cnu_api.plus_url  
            notice_path = cnu_api.page_urls["notices"]
        
        url = base_url + notice_path
        result = cnu_api.fetch_page_content(url)
        
        if "error" in result:
            return f"ê³µì§€ì‚¬í•­ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {result['error']}"
        
        # ê³µì§€ì‚¬í•­ íŒŒì‹±
        notices = []
        notices.append(f"ðŸ“¢ ì¶©ë‚¨ëŒ€í•™êµ {source} ê³µì§€ì‚¬í•­")
        notices.append("=" * 50)
        
        content = result["content"]
        
        # ê³µì§€ì‚¬í•­ íŒ¨í„´ ê²€ìƒ‰ (ì¼ë°˜ì ì¸ ê²Œì‹œíŒ í˜•íƒœ)
        notice_patterns = [
            r"(\d{4}[-./]\d{1,2}[-./]\d{1,2}).*?([^\n]{10,100})",
            r"([^\n]{10,100}).*?(\d{4}[-./]\d{1,2}[-./]\d{1,2})",
        ]
        
        found_notices = []
        for pattern in notice_patterns:
            matches = re.findall(pattern, content)
            if matches:
                found_notices.extend(matches[:max_results])
                break
        
        if found_notices:
            for i, (date_or_title, title_or_date) in enumerate(found_notices[:max_results], 1):
                # ë‚ ì§œì™€ ì œëª© êµ¬ë¶„
                if re.match(r"\d{4}[-./]\d{1,2}[-./]\d{1,2}", date_or_title):
                    date, title = date_or_title, title_or_date
                else:
                    title, date = date_or_title, title_or_date
                
                notices.append(f"{i}. {title.strip()}")
                notices.append(f"   ðŸ“… {date.strip()}")
                notices.append("")
        else:
            # ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€
            notices.extend([
                "ðŸ” ìµœì‹  ê³µì§€ì‚¬í•­ì„ í™•ì¸í•˜ë ¤ë©´ ë‹¤ìŒ ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•˜ì„¸ìš”:",
                "",
                f"ðŸŒ {source} ê³µì§€ì‚¬í•­: {url}",
                "",
                "ì£¼ìš” í™•ì¸ì‚¬í•­:",
                "â€¢ ìˆ˜ê°•ì‹ ì²­/ì •ì • ê´€ë ¨ ê³µì§€",
                "â€¢ í•™ì‚¬ì¼ì • ë³€ê²½ ì•ˆë‚´", 
                "â€¢ ìž¥í•™ê¸ˆ ì‹ ì²­ ì•ˆë‚´",
                "â€¢ ì¡¸ì—… ê´€ë ¨ ê³µì§€",
                "â€¢ ê°ì¢… í–‰ì‚¬ ë° í”„ë¡œê·¸ëž¨ ì•ˆë‚´"
            ])
        
        return "\n".join(notices)
        
    except Exception as e:
        return f"ê³µì§€ì‚¬í•­ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def search_cnu_site(query: str, site: str = "plus", max_results: int = 5) -> str:
    """
    ì¶©ë‚¨ëŒ€í•™êµ ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        site: ê²€ìƒ‰í•  ì‚¬ì´íŠ¸ (plus, ai)
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìžì—´
    """
    try:
        if site == "ai":
            site_url = "ai.cnu.ac.kr"
            site_name = "ì¸ê³µì§€ëŠ¥í•™ê³¼"
        else:
            site_url = "plus.cnu.ac.kr"
            site_name = "ì¶©ë‚¨ëŒ€í•™êµ"

        # ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰ (site: ì—°ì‚°ìž í™œìš©)
        search_query = f"site:{site_url} {query}"
        
        result = web_search.search_web(search_query, max_results)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        search_info = []
        search_info.append(f"ðŸ” {site_name} ì‚¬ì´íŠ¸ ê²€ìƒ‰ ê²°ê³¼")
        search_info.append(f"ê²€ìƒ‰ì–´: {query}")
        search_info.append("=" * 50)
        search_info.append("")
        search_info.append(result)
        
        return "\n".join(search_info)
        
    except Exception as e:
        return f"ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# í…ŒìŠ¤íŠ¸ ë° ì˜ˆì‹œ
if __name__ == '__main__':
    print("ì¶©ë‚¨ëŒ€í•™êµ Functions í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    functions_to_test = [
        ("ê³µì§€ì‚¬í•­ ì¡°íšŒ", lambda: get_cnu_notices("ëŒ€í•™", 5)),
        ("ì‚¬ì´íŠ¸ ê²€ìƒ‰", lambda: search_cnu_site("ì¸ê³µì§€ëŠ¥í•™ê³¼ êµê³¼ê³¼ì •", "plus", 3))
    ]
    
    for name, func in functions_to_test:
        print(f"\nðŸ“‹ {name} í…ŒìŠ¤íŠ¸:")
        print("-" * 30)
        try:
            result = func()
            print(result[:500] + "..." if len(result) > 500 else result)
        except Exception as e:
            print(f"ì˜¤ë¥˜: {e}")
        print()
